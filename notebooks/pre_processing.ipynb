{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f923063",
      "metadata": {
        "id": "2f923063"
      },
      "source": [
        "# **3. Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f3eacb",
      "metadata": {
        "id": "e8f3eacb"
      },
      "outputs": [],
      "source": [
        "!pip install word2number\n",
        "!pip install emoji\n",
        "!pip install pyspellchecker\n",
        "!pip install wordcloud\n",
        "!pip install wordninja\n",
        "!pip install langdetect\n",
        "!pip install nltk\n",
        "!pip install -U deep-translator\n",
        "!pip install emojis\n",
        "!pip install -U sentence-transformers\n",
        "!pip install zeugma\n",
        "!pip install --upgrade category_encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5738e0dc",
      "metadata": {
        "id": "5738e0dc"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167a3290",
      "metadata": {
        "id": "167a3290"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# text pre-processing \n",
        "import re, string #library that contains punctuation\n",
        "from word2number import w2n #Convert numeric words to numbers\n",
        "import wordninja # Split attached words\n",
        "from langdetect import detect # Language detection\n",
        "from langdetect import DetectorFactory # enforce consistent results for lang detection\n",
        "DetectorFactory.seed = 0 # For consistent language detection\n",
        "from deep_translator import GoogleTranslator # translator\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # To remove stopwords\n",
        "from nltk.tokenize import word_tokenize # Tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer #Stemming \n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer # Lemmatization\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # for calculating similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity # Cosine similarity\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "# # To create word cloud\n",
        "from wordcloud import WordCloud \n",
        "from wordcloud import STOPWORDS\n",
        "from PIL import Image\n",
        "from wordcloud import ImageColorGenerator\n",
        "\n",
        "# To check difference between similarity\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import skew # Skewness for normality\n",
        "from scipy.stats import kurtosis # Kurtosis value of the normal distribution\n",
        "# For word/sentence embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from zeugma.embeddings import EmbeddingTransformer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb46781a",
      "metadata": {
        "id": "cb46781a"
      },
      "source": [
        "## **3 Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ybs3ZZaVz8JT",
      "metadata": {
        "id": "ybs3ZZaVz8JT"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qz1Q2w9YC8Tk",
      "metadata": {
        "id": "Qz1Q2w9YC8Tk"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('data_preprocess.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c214bb44",
      "metadata": {
        "id": "c214bb44"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065eb43e",
      "metadata": {
        "id": "065eb43e"
      },
      "outputs": [],
      "source": [
        "df.iloc[:2, 100:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e033ddcd",
      "metadata": {
        "id": "e033ddcd"
      },
      "source": [
        "## **3.1 Merge Title and Review Body**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78192cd6",
      "metadata": {
        "id": "78192cd6"
      },
      "source": [
        "**Note:** There are two features that can be used in NLP pre-processing.\n",
        "* Text: Title and body of review\n",
        "* Description: Explanation of application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de348639",
      "metadata": {
        "id": "de348639"
      },
      "outputs": [],
      "source": [
        "# create a new feature as 'text'\n",
        "df['text'] = df['title'] + ' ' + df['body']\n",
        "df['text'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb49b52b",
      "metadata": {
        "id": "fb49b52b"
      },
      "source": [
        "## **3.2 Create Additional Features Related to Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bfa0b89",
      "metadata": {
        "id": "3bfa0b89"
      },
      "outputs": [],
      "source": [
        "# Number of hastags\n",
        "df['num_hashtags_text'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "df['num_hashtags_desc'] = df['description'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# number of mentions \n",
        "df['num_ment_text'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@'])) \n",
        "df['num_ment_desc'] = df['description'].apply(lambda x: len([c for c in str(x) if c == '@'])) \n",
        "\n",
        "# Number of words\n",
        "df['num_word_text'] = df['text'].apply(lambda x: len(str(x).split()))   \n",
        "df['num_word_desc'] = df['description'].apply(lambda x: len(str(x).split()))  \n",
        "\n",
        "# Number of stopwords\n",
        "df['num_s_word_text'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])) \n",
        "df['num_s_word_desc'] = df['description'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS])) \n",
        "\n",
        "# Number of URLs\n",
        "df['num_url_text'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))    \n",
        "df['num_url_desc'] = df['description'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))  \n",
        "\n",
        "# Average word length\n",
        "df['ave_word_text'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))   \n",
        "df['ave_word_desc'] = df['description'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))   \n",
        "\n",
        "# Number of punctuation\n",
        "df['num_punc_text'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation])) \n",
        "df['num_punc_desc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92df804e",
      "metadata": {
        "id": "92df804e"
      },
      "outputs": [],
      "source": [
        "df.iloc[:2, 114:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e63dc15",
      "metadata": {
        "id": "2e63dc15"
      },
      "source": [
        "##  **3.3 The Length of Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe06879a",
      "metadata": {
        "id": "fe06879a"
      },
      "source": [
        "**Note:** There is a column in df as 'length'. I want to check whether this column is same as length of **text** column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a9e655",
      "metadata": {
        "id": "42a9e655"
      },
      "outputs": [],
      "source": [
        "df['len'] = df['text'].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8776b9",
      "metadata": {
        "id": "6a8776b9"
      },
      "outputs": [],
      "source": [
        "((df['length'] - df['len']) != 0).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269fa9a8",
      "metadata": {
        "id": "269fa9a8"
      },
      "source": [
        "**Note:** When I merged 'title' and 'body' column I added a space. So there is one more character in all 16000 cases. These two columns are same. I can drop df['len'] column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08a2e3f",
      "metadata": {
        "id": "d08a2e3f"
      },
      "outputs": [],
      "source": [
        "df.drop(columns = 'len', inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be164697",
      "metadata": {
        "id": "be164697"
      },
      "source": [
        "## **3.4 Detect Language**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j6pRQOdSi-hi",
      "metadata": {
        "id": "j6pRQOdSi-hi"
      },
      "source": [
        "I will detect the language of both 'review text' and 'app description' features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ebee99",
      "metadata": {
        "id": "16ebee99"
      },
      "source": [
        "### **3.4.1 Language Detection of Review Text**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6773c4e0",
      "metadata": {
        "id": "6773c4e0"
      },
      "outputs": [],
      "source": [
        "# Create a new column \n",
        "df[\"lang_text\"] = np.nan\n",
        "\n",
        "# detect language for each review text\n",
        "for i in range(len(df)):\n",
        "    try: \n",
        "        df.loc[i, 'lang_text'] = detect(df.loc[i, 'text'])\n",
        "    except:\n",
        "        # Some cases includes just punction, emoji, number etc. In that case\n",
        "        # language cann't be detected. For these case create new variable as\n",
        "        # 'unknown'\n",
        "        df.loc[i, 'lang_text'] = 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f57c252",
      "metadata": {
        "id": "8f57c252"
      },
      "outputs": [],
      "source": [
        "# DENEME\n",
        "# Let's see the different languages in review text\n",
        "df['lang_text'].value_counts().head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9af5dde",
      "metadata": {
        "id": "b9af5dde"
      },
      "source": [
        "**Note:** A total of 15068 reviews were written in English. I will create a new feature that shows the language of review is in English or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d9f372",
      "metadata": {
        "id": "d4d9f372"
      },
      "outputs": [],
      "source": [
        "# Create a new feature that shows language is english or not\n",
        "df['english'] = np.where(df['lang_text'] == 'en',1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627644be",
      "metadata": {
        "id": "627644be"
      },
      "outputs": [],
      "source": [
        "# Check values of new column\n",
        "df['english'].value_counts()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wXBIk4H1v1AD",
      "metadata": {
        "id": "wXBIk4H1v1AD"
      },
      "outputs": [],
      "source": [
        "# calcuate the rate of reviews in English\n",
        "15068/16000*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7c1f6c",
      "metadata": {
        "id": "db7c1f6c"
      },
      "outputs": [],
      "source": [
        "df[['text', 'lang_text']].loc[865]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4ee3bd",
      "metadata": {
        "id": "3e4ee3bd"
      },
      "source": [
        "### **3.4.2 Language Detection of App Description**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55e409af",
      "metadata": {
        "id": "55e409af"
      },
      "outputs": [],
      "source": [
        "# Create a new column \n",
        "df[\"lang_desc\"] = np.nan\n",
        "\n",
        "# detect language for each review text\n",
        "for i in range(len(df)):\n",
        "    try: \n",
        "        df.loc[i, 'lang_desc'] = detect(df.loc[i, 'description'])\n",
        "    except:\n",
        "        # Some cases includes just punction, emoji, number etc. In that case\n",
        "        # language cann't be detected. For these case create new variable as\n",
        "        # 'unknown'\n",
        "        df.loc[i, 'lang_desc'] = 'Unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad35c3a",
      "metadata": {
        "id": "8ad35c3a"
      },
      "outputs": [],
      "source": [
        "# Let's see the different languages in review text\n",
        "df['lang_desc'].value_counts().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66W-vuFmwJBT",
      "metadata": {
        "id": "66W-vuFmwJBT"
      },
      "outputs": [],
      "source": [
        "# calcuate the rate of reviews in English\n",
        "15791/16000*100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449a190d",
      "metadata": {
        "id": "449a190d"
      },
      "source": [
        "## **3.5 Text Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9f5366",
      "metadata": {
        "id": "5b9f5366"
      },
      "outputs": [],
      "source": [
        "# Check these columns\n",
        "df[['text', 'description']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71b1fb6",
      "metadata": {
        "id": "a71b1fb6"
      },
      "outputs": [],
      "source": [
        "# Check the types\n",
        "df[['text', 'description']].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eywvhIuKlfuc",
      "metadata": {
        "id": "eywvhIuKlfuc"
      },
      "outputs": [],
      "source": [
        "# Check NaNs\n",
        "df[['text', 'description']].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ePaPJ1VemFBj",
      "metadata": {
        "id": "ePaPJ1VemFBj"
      },
      "source": [
        "**Note:** Converting object to string will makes NaNs as string. I want to keep original column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sipmit8-lzKc",
      "metadata": {
        "id": "Sipmit8-lzKc"
      },
      "outputs": [],
      "source": [
        "# Make a copy of df['description'] column\n",
        "df['orig_description'] = df['description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4119ac96",
      "metadata": {
        "id": "4119ac96"
      },
      "outputs": [],
      "source": [
        "# Convert 'description' column to string\n",
        "df['description'] = df['description'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294a91f3",
      "metadata": {
        "id": "294a91f3"
      },
      "source": [
        "***Remove Noisy Text***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f566f969",
      "metadata": {
        "id": "f566f969"
      },
      "outputs": [],
      "source": [
        "# Define a function for removing noisy text\n",
        "def text_cleaning(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans('','',string.punctuation))\n",
        "    # remove HTML\n",
        "    text = re.compile(r'<.*?>').sub(r'',text)\n",
        "    # remove emoji\n",
        "    text = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                         u\"\\U00002702-\\U000027B0\"\n",
        "                         u\"\\U000024C2-\\U0001F251\"\n",
        "                         \"]+\", flags=re.UNICODE).sub(r'', text)\n",
        "    # remove URL\n",
        "    text = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'',text)\n",
        "    # remove white spaces\n",
        "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
        "    # remove text in the square brackets\n",
        "    text = re.sub('\\[[^]]*\\]','', text)\n",
        "    # remove line symbol\n",
        "    text = re.sub('\\n', '', text)\n",
        "    # remove words that contain numbers\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef96d85",
      "metadata": {
        "id": "5ef96d85"
      },
      "outputs": [],
      "source": [
        "# Removing punctuation with definede function\n",
        "df['text_clean'] = df['text'].apply(lambda x : text_cleaning(x))\n",
        "df['description_clean'] = df['description'].apply(lambda x : text_cleaning(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2a4629",
      "metadata": {
        "id": "dc2a4629"
      },
      "outputs": [],
      "source": [
        "# Check one of the case not in English\n",
        "df[['text', 'text_clean']].loc[669]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y2tZddWcda-g",
      "metadata": {
        "id": "y2tZddWcda-g"
      },
      "source": [
        "**Note:** The rate of noisy text has been created as a new feature for models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AzXgOHAWbqV_",
      "metadata": {
        "id": "AzXgOHAWbqV_"
      },
      "outputs": [],
      "source": [
        "df['remove_rate'] = round((df['text'].str.len() - df['text_clean'].str.len())/df['length']*100,2).sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc28cf7",
      "metadata": {
        "id": "3bc28cf7"
      },
      "source": [
        "## **3.6 Translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Gt3IASajhcB",
      "metadata": {
        "id": "3Gt3IASajhcB"
      },
      "source": [
        "As the previous section shows, some text is not written in English. I will translate the review text and app description into English."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c8e2fa4",
      "metadata": {
        "id": "6c8e2fa4"
      },
      "source": [
        "### **3.6.1 Translation of Reviews into English**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10309e26",
      "metadata": {
        "id": "10309e26"
      },
      "outputs": [],
      "source": [
        "# Let's see the languages\n",
        "df['lang_text'].value_counts().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7e9747",
      "metadata": {
        "id": "ab7e9747"
      },
      "outputs": [],
      "source": [
        "# create a blank column for the translated version\n",
        "df[\"text_trans\"] = np.nan\n",
        "\n",
        "# translate all cases which are not written in English\n",
        "for i in range(len(df)):\n",
        "    if df.loc[i, 'lang_text'] == 'en':\n",
        "        df.loc[i, 'text_trans'] = df.loc[i, 'text_clean'] # Put original version\n",
        "    elif  df.loc[i, 'lang_text'] != 'en':\n",
        "        df.loc[i, 'text_trans'] = GoogleTranslator(source='auto', target='en').translate(df.loc[i, 'text_clean']) # Put translated version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40315a32",
      "metadata": {
        "id": "40315a32"
      },
      "outputs": [],
      "source": [
        "# See some examples from text which are not English.\n",
        "df[df['lang_text'] != 'en'][['text_clean', 'text_trans']].sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RryHweWBCNt",
      "metadata": {
        "id": "6RryHweWBCNt"
      },
      "source": [
        "**Note:** Check whether there is NaNs in translated review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8E-RM_Y0_3mA",
      "metadata": {
        "id": "8E-RM_Y0_3mA"
      },
      "outputs": [],
      "source": [
        "# Is there any NANs in translated review text.\n",
        "df[df['text_trans'].isna()][['text',\t'text_clean', 'text_trans']]\n",
        "\n",
        "# There is one case"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5s_ObhRB-ju",
      "metadata": {
        "id": "c5s_ObhRB-ju"
      },
      "source": [
        "**Note:** Cleaning process delete all thumbs up but not the kiwi emoji. Let's correct this case with index 12374."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BFH4TTeEB9hl",
      "metadata": {
        "id": "BFH4TTeEB9hl"
      },
      "outputs": [],
      "source": [
        "# delete ðŸ¥ emoji\n",
        "df.loc[12374, 'text_trans'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OxYeVHDTCWR1",
      "metadata": {
        "id": "OxYeVHDTCWR1"
      },
      "outputs": [],
      "source": [
        "# Let's check\n",
        "df[df['text_trans'].isna()][['text',\t'text_clean', 'text_trans']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** There is no NaNs in review text."
      ],
      "metadata": {
        "id": "UMGyZoVGXwwW"
      },
      "id": "UMGyZoVGXwwW"
    },
    {
      "cell_type": "markdown",
      "id": "5fe91e8e",
      "metadata": {
        "id": "5fe91e8e"
      },
      "source": [
        "### **3.6.2 Translation of App Descriptions into English**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad697e06",
      "metadata": {
        "id": "ad697e06"
      },
      "outputs": [],
      "source": [
        "# create a blank column for translated version\n",
        "df[\"desc_trans\"] = np.nan\n",
        "\n",
        "# translate all cases which are not written in English\n",
        "for i in range(len(df)):\n",
        "    if df.loc[i, 'lang_desc'] == 'en':\n",
        "        df.loc[i, 'desc_trans'] = df.loc[i, 'description_clean'] # Put original review\n",
        "    elif  df.loc[i, 'lang_desc'] != 'en':\n",
        "        df.loc[i, 'desc_trans'] = GoogleTranslator(source='auto', target='en').translate(df.loc[i, 'description_clean']) # Put translated version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7d6a93",
      "metadata": {
        "id": "2e7d6a93"
      },
      "outputs": [],
      "source": [
        "# Get some examples for checking translation\n",
        "df[df['lang_desc'] != 'en'][['description_clean', 'desc_trans']].sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d02f36",
      "metadata": {
        "id": "d2d02f36"
      },
      "source": [
        "**Note:** There are 148 NaNs (as string) for application description. Translation process convert these NANs to 'in'. I have to convert these 'in' to ''. '' means there is no description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c177a66c",
      "metadata": {
        "id": "c177a66c"
      },
      "outputs": [],
      "source": [
        "# Check the number of 'in' in df\n",
        "df[df['desc_trans'] =='in'].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234dd61b",
      "metadata": {
        "id": "234dd61b"
      },
      "outputs": [],
      "source": [
        "# Replace 'in' with ''\n",
        "for i in range(len(df)):\n",
        "  if df.loc[i,'desc_trans'] == 'in':\n",
        "    df.loc[i,'desc_trans'] = '' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63001f91",
      "metadata": {
        "id": "63001f91"
      },
      "outputs": [],
      "source": [
        "# Check the number of 'in' in df\n",
        "print(df[df['desc_trans'] =='in'].shape[0])\n",
        "\n",
        "# Check NANs in df['desc_trans'] column\n",
        "df[df['desc_trans']=='']['desc_trans'].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iZSBBrRNQerk",
      "metadata": {
        "id": "iZSBBrRNQerk"
      },
      "source": [
        "## **3.7 Prepare Data to Find Similarity**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba00021",
      "metadata": {
        "id": "2ba00021"
      },
      "source": [
        "### **3.7.1 Remove Stopwords and Short Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090d32bd",
      "metadata": {
        "id": "090d32bd"
      },
      "outputs": [],
      "source": [
        "def stopwords_shortwords(text):\n",
        "    # filter out stop words\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words( 'english' ))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    for word in words:\n",
        "        if word.isalpha():\n",
        "            words = [word for word in words if len(word) > 1 ]\n",
        "        else:\n",
        "            words = [word for word in words]\n",
        "    return\" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d190c3ad",
      "metadata": {
        "id": "d190c3ad"
      },
      "outputs": [],
      "source": [
        "df['text_trans'] = df['text_trans'].apply(lambda x : stopwords_shortwords(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['desc_trans'] = df['desc_trans'].apply(lambda x : stopwords_shortwords(x))"
      ],
      "metadata": {
        "id": "pEPELJoR8TM_"
      },
      "id": "pEPELJoR8TM_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "106faf04",
      "metadata": {
        "id": "106faf04"
      },
      "source": [
        "### **3.7.2 Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IqWABhoE-cch",
      "metadata": {
        "id": "IqWABhoE-cch"
      },
      "outputs": [],
      "source": [
        "df['text_trans_token'] = df['text_trans'].apply(word_tokenize)\n",
        "df['desc_trans_token'] = df['desc_trans'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e250a5cd",
      "metadata": {
        "id": "e250a5cd"
      },
      "source": [
        "### **3.7.3 Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66640b95",
      "metadata": {
        "id": "66640b95"
      },
      "outputs": [],
      "source": [
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c45f95c",
      "metadata": {
        "id": "3c45f95c"
      },
      "outputs": [],
      "source": [
        "# Use defined function for stemming\n",
        "df['text_trans_token'] = df['text_trans_token'].apply(lambda x: stemming(x))\n",
        "df['desc_trans_token'] = df['desc_trans_token'].apply(lambda x: stemming(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-7tMU1gzDswH",
      "metadata": {
        "id": "-7tMU1gzDswH"
      },
      "source": [
        "## **3.8 Calculating Similarity Between Reviews and Descriptions**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Create a empty list for similarities\n",
        "similarity_list = []\n",
        "\n",
        "#Define TfIdfVectorizer\n",
        "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)  \n",
        "\n",
        "for r in range(len(df)): # Visit all rows\n",
        "  try:\n",
        "    # Df for each row's text and description\n",
        "    df_similarity = pd.DataFrame([[df['text_trans_token'].loc[r]], [df['desc_trans_token'].loc[r]]]\n",
        "                     , columns=['text'])\n",
        "    # Transform two rows of column\n",
        "    tdidf_vector = tfidf.fit_transform(df_similarity['text'])\n",
        "    # calculate similarity\n",
        "    similarities = cosine_similarity(tdidf_vector[0],tdidf_vector[1])\n",
        "    # Add to the list\n",
        "    similarity_list.append(similarities)\n",
        "  except:\n",
        "    # The cosine similarity is a number between 0 and 1\n",
        "    similarity_list.append(0.001)"
      ],
      "metadata": {
        "id": "56Z5-Zh_8wUs"
      },
      "id": "56Z5-Zh_8wUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append list as a new column to df\n",
        "df['similarity'] = similarity_list"
      ],
      "metadata": {
        "id": "bkwzkJTuaiZf"
      },
      "id": "bkwzkJTuaiZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the cases whose similarity is 0.001\n",
        "df[df['similarity']==0.001][['text_trans_token', 'desc_trans_token', 'similarity']]"
      ],
      "metadata": {
        "id": "LriMwsKra9YF"
      },
      "id": "LriMwsKra9YF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In these two cases both text and description is blank. "
      ],
      "metadata": {
        "id": "pTqEPX2LZF6M"
      },
      "id": "pTqEPX2LZF6M"
    },
    {
      "cell_type": "code",
      "source": [
        "# See the column\n",
        "df['similarity'].head()"
      ],
      "metadata": {
        "id": "rz1CB_qX6aXJ"
      },
      "id": "rz1CB_qX6aXJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are extra square brackets. Remove and convert them to float.\n",
        "df['similarity'] = df['similarity'].apply(lambda x: str(x).replace('[', '').replace(']',''))\n",
        "df['similarity'] = df['similarity'].astype(float)"
      ],
      "metadata": {
        "id": "H63NWURYVTxi"
      },
      "id": "H63NWURYVTxi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the column\n",
        "df['similarity'].dtype"
      ],
      "metadata": {
        "id": "8KjRR5InZby3"
      },
      "id": "8KjRR5InZby3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Let's explore the similarity difference between fake and real reviews."
      ],
      "metadata": {
        "id": "CZUCsBUjMuhl"
      },
      "id": "CZUCsBUjMuhl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics of total reviews\n",
        "df.groupby([\"label\"])[\"similarity\"].describe()"
      ],
      "metadata": {
        "id": "4z06o0zHMcJC"
      },
      "id": "4z06o0zHMcJC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use  skewness and kurtosis to measure the shape of a distribution.\n",
        "print('kurtosis:', kurtosis(df['similarity'], axis=0, bias=True))\n",
        "print('skewness:', skew(df['similarity'], axis=0, bias=True))"
      ],
      "metadata": {
        "id": "uaoXwXuDLuYO"
      },
      "id": "uaoXwXuDLuYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carry out two sample t test\n",
        "stats.ttest_ind(df['length'][df['label'] == 0],\n",
        "                df['length'][df['label'] == 1])"
      ],
      "metadata": {
        "id": "33L_Zen2L-i3"
      },
      "id": "33L_Zen2L-i3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b714f181",
      "metadata": {
        "id": "b714f181"
      },
      "source": [
        "## **3.9 Word Cloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eZokNZClDiFA",
      "metadata": {
        "id": "eZokNZClDiFA"
      },
      "outputs": [],
      "source": [
        "# Use in google.colab to upload .png \n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85f0903",
      "metadata": {
        "id": "d85f0903"
      },
      "outputs": [],
      "source": [
        "# Generate a word cloud image\n",
        "stopwords = set(STOPWORDS)\n",
        "mask = np.array(Image.open('appstore.png'))\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color = 'white', mode=\"RGBA\", max_words=1000, mask=mask).generate(' '.join(df['body']))\n",
        "\n",
        "# create coloring from image\n",
        "image_colors = ImageColorGenerator(mask)\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LxGq9hRFCJiK",
      "metadata": {
        "id": "LxGq9hRFCJiK"
      },
      "outputs": [],
      "source": [
        "# download word cloud image in colab\n",
        "wordcloud.to_file(\"word_cloud.png\")\n",
        "files.download('word_cloud.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a61141",
      "metadata": {
        "id": "11a61141"
      },
      "outputs": [],
      "source": [
        "# download word cloud image in jupyter\n",
        "'''from IPython.display import FileLink, FileLinks\n",
        "\n",
        "df.to_csv('/Users/handedede/Documents/GitHub/capstone_fake_review/data.csv', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.10 Word to Vectors**"
      ],
      "metadata": {
        "id": "eb8PVvN1Vp3W"
      },
      "id": "eb8PVvN1Vp3W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In this part, Glove word embedding is used for converting words to vectors. There is a package called Zeugma that helps to covert."
      ],
      "metadata": {
        "id": "UnoYBojIgctw"
      },
      "id": "UnoYBojIgctw"
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformer(zeugma)\n",
        "glove_review = EmbeddingTransformer('glove')"
      ],
      "metadata": {
        "id": "ScBH5X3DVwCG"
      },
      "id": "ScBH5X3DVwCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Transformation of review text***"
      ],
      "metadata": {
        "id": "h-ZsOEdXHOup"
      },
      "id": "h-ZsOEdXHOup"
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_trans'].sample(5)"
      ],
      "metadata": {
        "id": "3dH7bEAuhf1V"
      },
      "id": "3dH7bEAuhf1V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert review text column\n",
        "X_text_glove = glove_review.transform(df['text_trans']) "
      ],
      "metadata": {
        "id": "fL88A0DdhNh8"
      },
      "id": "fL88A0DdhNh8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep it into df\n",
        "df_text_glove = pd.DataFrame(X_text_glove)\n",
        "df_text_glove.head()"
      ],
      "metadata": {
        "id": "k8QAoPgIhov8"
      },
      "id": "k8QAoPgIhov8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Transformation of app description***"
      ],
      "metadata": {
        "id": "XkqMa6zzHUFs"
      },
      "id": "XkqMa6zzHUFs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert description columns\n",
        "X_desc_glove = glove_review.transform(df['desc_trans'])"
      ],
      "metadata": {
        "id": "NMI_srcMh7cw"
      },
      "id": "NMI_srcMh7cw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep it into df\n",
        "df_desc_glove = pd.DataFrame(X_desc_glove)\n",
        "df_desc_glove.head()"
      ],
      "metadata": {
        "id": "9--uOn-RiDe5"
      },
      "id": "9--uOn-RiDe5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download both df\n",
        "df_text_glove.to_csv('df_text_glove.csv', encoding = 'utf-8-sig') \n",
        "files.download('df_text_glove.csv')"
      ],
      "metadata": {
        "id": "R1gvgwHtiZYi"
      },
      "id": "R1gvgwHtiZYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.11 Sentence to Vectors**"
      ],
      "metadata": {
        "id": "cuX-BnA3VwoU"
      },
      "id": "cuX-BnA3VwoU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We are implementing pre-trained BERT model which is all about the context of a word in a sentence. It is a good idea to use so because the Bert's embeddings have been trained on huge text data, beyond we could accomplish with this small dataset of reviews. It is therefore much more efficient and accurate."
      ],
      "metadata": {
        "id": "4wkajbh4bcVt"
      },
      "id": "4wkajbh4bcVt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Bricken ([2021](https://bricken.co/nlp_disaster_tweets_2/)) has found that heavy text data cleaning works worse when input into a BERT model because this contextual information is lost. Therefore, we have used the raw form of textual data (â€˜textâ€™ and â€˜descriptionâ€™)."
      ],
      "metadata": {
        "id": "TnnQinH4daJc"
      },
      "id": "TnnQinH4daJc"
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'].sample(5)"
      ],
      "metadata": {
        "id": "0QcjU-OweN4H"
      },
      "id": "0QcjU-OweN4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "MI3f2tS7eSDD"
      },
      "id": "MI3f2tS7eSDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Convert review text to vectors***"
      ],
      "metadata": {
        "id": "AUbahAode0x_"
      },
      "id": "AUbahAode0x_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all reviews in a list\n",
        "reviews = df['text'][:].values\n",
        "print(len(reviews))"
      ],
      "metadata": {
        "id": "ddrMRtGkeUfF"
      },
      "id": "ddrMRtGkeUfF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentence to vectors\n",
        "review_embeddings_base = model.encode(reviews)"
      ],
      "metadata": {
        "id": "dzJDIoCZeYiv"
      },
      "id": "dzJDIoCZeYiv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df for vectors \n",
        "df_text_bert = pd.DataFrame(review_embeddings_base)"
      ],
      "metadata": {
        "id": "2Uo9-pF1edZz"
      },
      "id": "2Uo9-pF1edZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Convert app descriptions to vectors***"
      ],
      "metadata": {
        "id": "vILkt5lxfCB8"
      },
      "id": "vILkt5lxfCB8"
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'].isna().sum()"
      ],
      "metadata": {
        "id": "d8s7qRwP7Qf9"
      },
      "id": "d8s7qRwP7Qf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all reviews in a list\n",
        "description = df['description'][:].values\n",
        "print(len(description))"
      ],
      "metadata": {
        "id": "8J7_kVr2fCB8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8J7_kVr2fCB8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentence to vectors\n",
        "description_embeddings_base = model.encode(description)"
      ],
      "metadata": {
        "id": "kCa5oWaHfCB8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kCa5oWaHfCB8"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df for vectors \n",
        "df_desc_bert = pd.DataFrame(description_embeddings_base)"
      ],
      "metadata": {
        "id": "nw6dx72OfCB8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nw6dx72OfCB8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download both df\n",
        "df_text_bert.to_csv('df_text_bert.csv', encoding = 'utf-8-sig') \n",
        "df_desc_bert.to_csv('df_desc_bert.csv', encoding = 'utf-8-sig') \n",
        "files.download('df_text_bert.csv')\n",
        "files.download('df_desc_bert.csv')"
      ],
      "metadata": {
        "id": "03YY_4OqIUrI"
      },
      "id": "03YY_4OqIUrI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.12 Data Final Check Before Modelling**"
      ],
      "metadata": {
        "id": "mZFsewVWUbt0"
      },
      "id": "mZFsewVWUbt0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** This [document](https://docs.google.com/document/d/17Xe9_NeUw4Xr1bXgHiDy1-zQT4eQ-BwDaPqQkAKby-4/edit?usp=sharing) shows the last version of features in dataframe. I check all .dtypes and NaNs, encode the categorical data type and decided  which ones to use. I will encode after train-test split."
      ],
      "metadata": {
        "id": "hpf7gKUNZ7Sm"
      },
      "id": "hpf7gKUNZ7Sm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Edit some columns to use in modellings***"
      ],
      "metadata": {
        "id": "kmYqkMKR1o9k"
      },
      "id": "kmYqkMKR1o9k"
    },
    {
      "cell_type": "code",
      "source": [
        "# df['time_diff_release_post'] in datatime. Only use days\n",
        "print(df['time_diff_release_post'].sample(1))\n",
        "# Take just days off from datetime object \n",
        "df['time_diff_release_post']=df['time_diff_release_post'].apply(lambda x: x.split()[0]) "
      ],
      "metadata": {
        "id": "FqjEauiJVHAE"
      },
      "id": "FqjEauiJVHAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type\n",
        "print('Before', df['time_diff_release_post'].dtype)\n",
        "# convert to integer\n",
        "df['time_diff_release_post'] = df['time_diff_release_post'].astype('int')\n",
        "# Check the type\n",
        "print('After', df['time_diff_release_post'].dtype)"
      ],
      "metadata": {
        "id": "IafiJmPszU_r"
      },
      "id": "IafiJmPszU_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['user_account_usage'] is in seconds\n",
        "print(df['user_account_usage'].sample(1))\n",
        "# Concert seconds into days\n",
        "# 86400 seconds = 1 day\n",
        "df['user_account_usage_days'] = df['user_account_usage']//86400"
      ],
      "metadata": {
        "id": "whc1iH3VVIfa"
      },
      "id": "whc1iH3VVIfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type\n",
        "df['user_account_usage'].dtype"
      ],
      "metadata": {
        "id": "ILfdJ9kizslT"
      },
      "id": "ILfdJ9kizslT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are in seconds in df['diff_init_curr_2'] column\n",
        "print(df['diff_init_curr_2'].sample(1))\n",
        "print('Number of NaNs:', df['diff_init_curr_2'].isna().sum())\n",
        "# Take just days off from datetime object\n",
        "for r in range(len(df)):\n",
        "  try:\n",
        "     df['diff_init_curr_2'].iloc[r] = str(df['diff_init_curr_2'].iloc[r]).split()[0]\n",
        "  except:   # This is for NANs\n",
        "    df['diff_init_curr_2'].iloc[r] = df['diff_init_curr_2'].iloc[r]\n",
        "# Check the type\n",
        "print(df['diff_init_curr_2'].dtype)\n",
        "# Convert nan string to np.nan\n",
        "df['diff_init_curr_2'] = df['diff_init_curr_2'].replace('nan', np.nan).sort_values()\n",
        "# Check the number of NaNs\n",
        "print('Number of NaNs:', df['diff_init_curr_2'].isna().sum())"
      ],
      "metadata": {
        "id": "U62LI6xyVLDO"
      },
      "id": "U62LI6xyVLDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data for Models Notebook"
      ],
      "metadata": {
        "id": "cfFiDB9-VC0Z"
      },
      "id": "cfFiDB9-VC0Z"
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "sceeuEetQbdE"
      },
      "id": "sceeuEetQbdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873f37ca",
      "metadata": {
        "id": "873f37ca"
      },
      "outputs": [],
      "source": [
        "#Download the clean tokenized data\n",
        "df.to_csv('df_models.csv', encoding = 'utf-8-sig') \n",
        "files.download('df_models.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5fe91e8e"
      ],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}